{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFpmr-8It5V9"
      },
      "source": [
        "# 1 线性回归\n",
        "学习目标：\n",
        "1. 解释损失函数及其工作原理。\n",
        "2. 定义并描述梯度下降如何找到最佳模型参数。\n",
        "3. 描述如何调整超参数以有效训练线性模型。\n",
        "## 1.1 Linear Regression\n",
        "$$y = wx + b$$\n",
        " $w$ : weight(权重);\n",
        " $b$ : bias\n",
        "还有多个变量的，像：\n",
        "$$y = w_1x_1 +w_2x_2 +w_3x_3 + b$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "## 1.2 Loss\n",
        "### 1.2.1 Loss的度量（不关心正负）\n",
        "### 1.2.2 Type of loss\n",
        "| loss type | Def | Equation |\n",
        "| --------- | --- | -------- |\n",
        "|L1 loss    |模拟值和真实值的绝对值之和|$\\Sigma\\|actual_value - predicted_value\\|$|\n",
        "|MAE        |L1 loss的平均值|$\\frac{1}{N}\\Sigma\\|actual_value - predicted_value\\|$|\n",
        "|L2 loss    |模拟值和真实值的平方之和|$\\Sigma(actual_value - predicted_value)^2$|\n",
        "|MSE        |L2 loss的平均值|$\\frac{1}{N}\\Sigma(actual_value - predicted_value)^2$|\n",
        "\n",
        "### 1.2.3 MAE和MSE的区别\n",
        "主要体现在异常值对于整体计算的影响，相同的MSE和MAE下，MSE模拟的线性函数更接近异常值"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 梯度下降\n",
        "梯度下降是一种降低loss的数学方法。通过以下的迭代过程以找到最符合真实值的w和b。\n",
        "一开始会将weight和bias设置在0附近的随机值。\n",
        "\n",
        "1. 使用当前权重和偏差计算损失\n",
        "2. 确定可以降低loss移动权重和偏差的方向\n",
        "3. 在此方向上增加较小的一个值\n",
        "4. 重复第一步直到不能继续减小loss\n",
        "\n",
        "### 1.3.1 具体的例子\n",
        "\n",
        "在具体层面上，我们可以使用一个小数据集来完成梯度下降步骤，其中包含七个汽车重量（以磅为单位）及其每加仑行驶英里数的示例：\n",
        "|Pound(1000) (p)feature\t    |Miles per gallon (m)label|\n",
        "|---------------------------|------------------------ |\n",
        "|3.5\t                    |18                       |\n",
        "|3.69\t                    |15                       |\n",
        "|3.44\t                    |18                       |\n",
        "|3.43\t                    |16                       |\n",
        "|4.34\t                    |15                       |\n",
        "|4.42\t                    |14                       |\n",
        "|2.37\t                    |24                       |\n",
        "\n",
        "1. 开始的时候，将weight和bias设置成0:\n",
        "    $$weight=0$$\n",
        "    $$bias=0$$\n",
        "    $$m=0+0p$$\n",
        "2. 计算 MSE Loss:\n",
        "    $$MSE Loss = \\frac{1}{N}\\Sigma(m-m_0)^2 = 303.71$$\n",
        "3. 计算该参数下的Loss的斜率:\n",
        "    $$k_w=\\frac{\\sigma}{\\sigma{w}}MSE loss=-199.7$$\n",
        "    $$k_b=\\frac{\\sigma}{\\sigma{b}}MSE loss=-34.3$$\n",
        "4. 向负斜率方向移动少量以获得下一个权重和偏差。现在，我们将“少量”任意定义为 0.01:\n",
        "    $$w_1=w_0-0.01*k_w$$\n",
        "    $$b_1=b_0-0.01*k_b$$\n",
        "使用新的权重和偏差来计算损失并重复。完成六次迭代的过程，我们将得到以下权重、偏差和损失:\n",
        "\n",
        "|Iteration\t|Weight\t|Bias\t|Loss (MSE)|\n",
        "|-----------|-------|-------|----------|\n",
        "|1\t        |0\t    |0\t    |303.71|\n",
        "|2\t        |1.2\t|0.34\t|170.67|\n",
        "|3\t        |2.75\t|0.59\t|67.3|\n",
        "|4\t        |3.17\t|0.72\t|50.63|\n",
        "|5\t        |3.47\t|0.82\t|42.1|\n",
        "|6\t        |3.68\t|0.9\t|37.74|\n",
        "\n",
        "### 1.3.2 模型收敛和损失曲线\n",
        "### 1.3.3 收敛和凸函数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 超参数\n",
        "超参数Hyperparameters是控制训练不同方面的变量。常见的有三种:\n",
        "1. learning rate 学习率\n",
        "2. batch size 批大小\n",
        "3. epochs :整个训练数据集被遍历一次的过程\n",
        "### 1.4.1 超参数和参数之间的区别\n",
        "1. 参数:如w、b的值是可以训练出来的\n",
        "2. 超参数:是人为设置的\n",
        "### 1.4.2 learning rate\n",
        "学习率可以影响模型熟练的速度。如果学习率较低，则需要花费多次迭代才能收敛。若太高则无法收敛而是来回跳动。因此要选择不高也不低的学习率。\n",
        "### 1.4.3 batch size\n",
        "在深度学习中，我们通常不会一次性使用所有的训练数据来更新模型权重，而是将数据分成若干个批次，每个批次包含一定数量的样本。这样，模型可以在每个批次上进行权重更新，从而逐步优化其性能。Batch Size的大小通常设置为2的n次幂，如64、128、256等，这主要是为了充分利用计算机的内存和计算资源。\n",
        "在更新权重和偏差之前无需查看数据集中的每个示例即可平均获得正确梯度的两种常见技术是**随机梯度下降**和**小批量随机梯度下降**：\n",
        "1. 随机梯度下降:stochastic gradient descent(**SGD**)\n",
        "    每次使用随机的一个数据点训练。\n",
        "    但会产生噪声。请注意，使用随机梯度下降可能会在整个损失曲线上产生噪声，而不仅仅是在收敛附近。\n",
        "2. 小批量随机梯度下降:mini-batch stochastic gradient descent(**mini-batch SGD**)\n",
        "    就是不使用整个训练集，也不只使用一个数据点。\n",
        "### 1.4.4 epochs\n",
        "就是把每一个数据点都迭代过一遍，即使batch size不是整个训练集。具体如下\n",
        "|Batch type\t|When weights and bias updates occur|\n",
        "|-----------|-----------------------------------|\n",
        "|Full batch\t|After the model looks at all the examples in the dataset. For instance, if a dataset contains 1,000 examples and the model trains for 20 epochs, the model updates the weights and bias 20 times, once per epoch.|\n",
        "|Stochastic gradient descent\t|After the model looks at a single example from the dataset. For instance, if a dataset contains 1,000 examples and trains for 20 epochs, the model updates the weights and bias 20,000 times.|\n",
        "|Mini-batch stochastic gradient descent\t|After the model looks at the examples in each batch. For instance, if a dataset contains 1,000 examples, and the batch size is 100, and the model trains for 20 epochs, the model updates the weights and bias 200 times.|"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO97KQnx05EihztbTEN2zRI",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
